{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üç´ Creative common license dataset\n",
    "\n",
    "> ‚ö†Ô∏è Please note that this notebook is not compatible with Google Colab. To complete the tutorial, you must initiate Docker containers. Starting Docker containers within Google Colab is not supported.\n",
    "\n",
    "This sample pipeline demonstrates how to effectively utilize a creative\n",
    "commons image dataset within a fondant pipeline. This dataset comprises images from diverse sources and is available in various data formats.\n",
    "[The dataset](https://huggingface.co/datasets/fondant-ai/fondant-cc-25m) itself is available on Huggingface.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Pipeline overview\n",
    "\n",
    "The primary goal of this sample is to showcase how you can use a Fondant pipeline and reusable\n",
    "components to load an image dataset from HuggingFace Hub and download all images.\n",
    "Pipeline Steps:\n",
    "\n",
    "- [Load from Huggingface Hub](https://github.com/ml6team/fondant/tree/main/components/load_from_hf_hub):\n",
    "  The pipeline begins by loading the image dataset from Huggingface Hub.\n",
    "- [Download Images](https://github.com/ml6team/fondant/tree/main/components/download_images): \n",
    "  The download image component download images and stores them to parquet. \n",
    "- [Filter Images](https://github.com/ml6team/fondant/tree/main/components/filter_image_resolution):\n",
    "  The filter image component filters images based on their resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "### This section checks the prerequisites of your environment. Read any errors or warnings carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensure a Python between version 3.8 and 3.10 is available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info < (3, 8, 0) or sys.version_info >= (3, 11, 0):\n",
    "    raise Exception(f\"A Python version between 3.8 and 3.10 is required. You are running {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if docker compose is installed and the docker daemon is running**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose version >/dev/null\n",
    "!docker info >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Fondant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the pipeline\n",
    "\n",
    "First of all, we need to initialize the pipeline, which includes specifying a name for your pipeline, providing a description, and setting a base_path. The base_path is used to store the pipeline artifacts and data generated by the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pyarrow as pa\n",
    "\n",
    "from fondant.pipeline import Pipeline\n",
    "\n",
    "BASE_PATH = \"./fondant-artifacts\"\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "Path(BASE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=\"filter-creative-commons\",  # Add a unique pipeline name to easily track your progress and data\n",
    "    description=\"Load cc image dataset\",\n",
    "    base_path=BASE_PATH, # The demo pipelines uses a local directory to store the data.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we will utilize a dataset available on HuggingFace. As such, we will use a reusable Fondant component `load_from_hf_hub`. The `load_from_hf_hub` component is a generic one, which implies that we still need to specify the produce section of the component (explicitly name and type the fields te component will generate, you find the available fields on the [huggingface dataset](https://huggingface.co/datasets/fondant-ai/fondant-cc-25m)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following to your pipeline file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from hub component\n",
    "raw_data = pipeline.read(\n",
    "    \"load_from_hf_hub\",\n",
    "    arguments={\n",
    "        \"dataset_name\": \"fondant-ai/fondant-cc-25m\",\n",
    "        \"n_rows_to_load\": 100,  # Modify the number of images you want to download.\n",
    "    },\n",
    "    produces={\n",
    "        \"alt_text\": pa.string(),\n",
    "        \"image_url\": pa.string(),\n",
    "        \"license_location\": pa.string(),\n",
    "        \"license_type\": pa.string(),\n",
    "        \"webpage_url\": pa.string(),\n",
    "        \"surt_url\": pa.string(),\n",
    "        \"top_level_domain\": pa.string(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, our pipeline comprises a single component responsible for loading the dataset from the HuggingFace Hub. We have the flexibility to include additional components in the pipeline. In this instance, our objective is to download all the images. For this purpose, we will employ a reusable component named `download_images`. We apply the `download_images` component to the `load_from_hf_hub` component. This way we tell Fondant that this step should follow the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images component\n",
    "images = raw_data.apply(\n",
    "    \"download_images\",\n",
    "    arguments={\n",
    "        \"input_partition_rows\": 100,\n",
    "        \"resize_mode\": \"no\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reusable components offer various arguments that typically affect the component's operations. In this case, we have set, for example, `\"resize_mode\": \"no\"`. This setting ensures that the images will not be resized after they are downloaded. If you would like to learn more about components and their arguments, please refer to our [documentation](https://fondant.ai) and explore the [ComponentHub](https://hub.fondant.ai)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add one more step to our pipeline. We will filter the downloaded images based on their resolution. We will use the `filter_image_resolution` component for this purpose. This component requires two arguments: `min_width` and `max_aspect_ratio`. We will set these arguments so only images with a minimum resolution of 512 pixels will be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter images component\n",
    "big_images = images.apply(\n",
    "    \"filter_image_resolution\",\n",
    "    arguments={\n",
    "        \"min_image_dim\": 512,\n",
    "        \"max_aspect_ratio\": 2.5,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline\n",
    "\n",
    "Now we are ready to execute our pipeline. \n",
    "Fondant provides various executors, and in this case, we are using the LocalRunner, which utilizes Docker under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using a MacBook with a M1 processor you have to make sure to set the docker default platform to linux/amd64\n",
    "import os\n",
    "os.environ[\"DOCKER_DEFAULT_PLATFORM\"]=\"linux/amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.pipeline.runner import DockerRunner\n",
    "\n",
    "DockerRunner().run(input=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset\n",
    "\n",
    "You can also explore the dataset using the fondant explorer, this enables you to visualize your output dataset at each component step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(\n",
    "    base_path=BASE_PATH,\n",
    "    container=\"fndnt/data_explorer\",\n",
    "    tag=\"latest\",\n",
    "    port=8501\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
